{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics -----------------------------------------------------\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Word2vec ----------------------------------------------------\n",
    "import gensim\n",
    "\n",
    "# NLTK -----------------------------------------------------\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# Keras -----------------------------------------------\n",
    "\n",
    "# from keras.layers import Dense\n",
    "# from keras.models import Sequential\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_file = open('/media/polichinel/DATA/backup/PDS_DATA/time_lines.pkl', 'rb') # from feature set.\n",
    "\n",
    "# time_lines = pickle.load(pkl_file)\n",
    "\n",
    "# pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polichinel/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (0,1,4,7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#df_unlabled_big = pd.read_csv(\"/media/polichinel/DATA/backup/PDS_DATA/new_many_tweets.csv\", index_col= 0)\n",
    "df_unlabled_big1 = pd.read_csv(\"new_all_unlabeled_tweets1.csv\", index_col= 0)\n",
    "df_unlabled_big2 = pd.read_csv(\"new_all_unlabeled_tweets2.csv\", index_col= 0)\n",
    "\n",
    "df_unlabled = pd.read_csv(\"all_unlabled_tweets.csv\", index_col= 0)\n",
    "df_labled = pd.read_csv(\"labled_tweets.csv\", index_col= 0)\n",
    "\n",
    "pkl_file = open('full_remove.pkl', 'rb') # from feature set.\n",
    "full_remove = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # should be enough - check before deleting the direct implimentation\n",
    "\n",
    "# pkl_file = open('prep2.pkl', 'rb') # from feature set.\n",
    "# prep2 = pickle.load(pkl_file)\n",
    "# pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from prerpossecing and feature selection\n",
    "\n",
    "stop_words = []#[\"goptaxscam\"] # for now no stopwords\n",
    "\n",
    "def prep2(text):\n",
    "    wordlist = nltk.word_tokenize(text)\n",
    "    wordlist = [lemmatizer.lemmatize(w.lower()) for w in wordlist]\n",
    "    wordlist = [w for w in wordlist if w not in full_remove and w not in stop_words]\n",
    "    return wordlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model(model, most_sim = \"president\"):\n",
    "    print(\"Most similar to {}:\\n\".format(most_sim))\n",
    "    for i in model.wv.most_similar(most_sim):\n",
    "        print(i)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # get the Most common words\n",
    "    print(\"Most common words:\")\n",
    "    print(model.wv.index2word[0], model.wv.index2word[1], model.wv.index2word[2])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # get the least common words\n",
    "    vocab_size = len(model.wv.vocab)\n",
    "    print(\"Least common words:\")\n",
    "    print(model.wv.index2word[vocab_size - 1], model.wv.index2word[vocab_size - 2], model.wv.index2word[vocab_size - 3])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # some similarity fun\n",
    "    print(\"Sanity sim check1:\")\n",
    "    print(model.wv.similarity('woman', 'man'), model.wv.similarity('woman', 'thing'))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Sanity sim check2:\")\n",
    "    # what doesn't fit?\n",
    "    print(model.wv.doesnt_match(\"green blue red man\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = [prep2(sent) for sent in nltk.sent_tokenize(df_unlabled_big1.text.to_string())]\n",
    "sentences2 = [prep2(sent) for sent in nltk.sent_tokenize(df_unlabled_big2.text.to_string())]\n",
    "sentences = sentences1 + sentences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195473\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24208"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('pres', 0.7571897506713867)\n",
      "('administration', 0.6175614595413208)\n",
      "('adminis', 0.5768076777458191)\n",
      "('president-elect', 0.5733792185783386)\n",
      "('administ', 0.5686612129211426)\n",
      "('admin', 0.565226674079895)\n",
      "('administratio', 0.5596060156822205)\n",
      "('administrat', 0.5422479510307312)\n",
      "('presi', 0.522792637348175)\n",
      "('donald', 0.5141134262084961)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "with you today\n",
      "\n",
      "\n",
      "Least common words:\n",
      "opportunityagenda hcsosheriff sfareachamber\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.27627604539667955 0.06572580374230565\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(df_unlabled.text.to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('trump', 0.9045068025588989)\n",
      "('obama', 0.8545442819595337)\n",
      "('donald', 0.84553062915802)\n",
      "('admin', 0.836225152015686)\n",
      "('mr.', 0.7751523852348328)\n",
      "('order', 0.7572319507598877)\n",
      "('decision', 0.7467522621154785)\n",
      "('administration', 0.7373746633529663)\n",
      "('executive', 0.735882043838501)\n",
      "('comey', 0.732078492641449)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "this with you\n",
      "\n",
      "\n",
      "Least common words:\n",
      "hazleton thewilsontimes ppact\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.6184895525381566 0.4613569059964778\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('word2vec_small_model.pkl', 'wb')\n",
    "pickle.dump(model, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/polichinel/DATA/backup/PDS_DATA/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hot_comments_politics.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"hot_comments_politics.pkl\"\n",
    "\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments1 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments1).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('donald', 0.920943021774292)\n",
      "('twitter', 0.8230994939804077)\n",
      "('team', 0.8208926320075989)\n",
      "('trump', 0.8097741007804871)\n",
      "('mr.', 0.808213472366333)\n",
      "('asked', 0.8029544353485107)\n",
      "('gowdy', 0.802731990814209)\n",
      "('trey', 0.799493670463562)\n",
      "('campaign', 0.7989790439605713)\n",
      "('putin', 0.7971084117889404)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that you this\n",
      "\n",
      "\n",
      "Least common words:\n",
      "biden agency parson\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.7974816789762588 0.5014077876406039\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### controversial_comments_politics.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4397"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"controversial_comments_politics.pkl\"\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments2 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments2).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('biden', 0.7018259763717651)\n",
      "('office', 0.6553453803062439)\n",
      "('running', 0.6535567045211792)\n",
      "('senate', 0.6323044300079346)\n",
      "('2012', 0.6304991245269775)\n",
      "('justice', 0.6250301003456116)\n",
      "('female', 0.6196533441543579)\n",
      "('tpp', 0.6081812381744385)\n",
      "('bush', 0.5983028411865234)\n",
      "('cand', 0.5941892266273499)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you that n't\n",
      "\n",
      "\n",
      "Least common words:\n",
      "h-1b infantry pyramid\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.4032415535639872 0.24680370158041864\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hot_comments_news.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"hot_comments_news.pkl\"\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments3 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments3).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('conservative', 0.9998845458030701)\n",
      "('freedom', 0.9998729825019836)\n",
      "('nah', 0.9998723268508911)\n",
      "('asking', 0.999870777130127)\n",
      "('depends', 0.999870777130127)\n",
      "('charge', 0.9998691082000732)\n",
      "('thanks', 0.9998683929443359)\n",
      "('con', 0.9998666644096375)\n",
      "('able', 0.9998666644096375)\n",
      "('obama', 0.9998661279678345)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you that n't\n",
      "\n",
      "\n",
      "Least common words:\n",
      "dealer preacher bluehole\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.9994451437714044 0.999103754053145\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### controversial_comments_news.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3174"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"controversial_comments_news.pkl\"\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments4 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments4).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('obama', 0.7636547684669495)\n",
      "('bush', 0.7594131827354431)\n",
      "('press', 0.7548365592956543)\n",
      "('wyden', 0.7422565817832947)\n",
      "('cupcake', 0.7402195930480957)\n",
      "('dr.', 0.7389487028121948)\n",
      "('camera', 0.7380211353302002)\n",
      "('mac', 0.7356566190719604)\n",
      "('kkk', 0.7342849969863892)\n",
      "('\\\\ni', 0.7300539016723633)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you that n't\n",
      "\n",
      "\n",
      "Least common words:\n",
      "dode blair tannerite\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.30876151574421223 0.1499693290056254\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political top comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10107"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"top_comments_politics.pkl\"\n",
    "\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments5 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments5).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('presi', 0.6865994930267334)\n",
      "('pres', 0.6773925423622131)\n",
      "('potus', 0.6593327522277832)\n",
      "('rogers', 0.46169722080230713)\n",
      "('arpaio', 0.420728862285614)\n",
      "('mrs.', 0.40820616483688354)\n",
      "('pardon', 0.40549448132514954)\n",
      "('nation', 0.40211716294288635)\n",
      "('presidential', 0.39559489488601685)\n",
      "('donald', 0.38100704550743103)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that this you\n",
      "\n",
      "\n",
      "Least common words:\n",
      "thermostat longmont equity\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.3178115025292681 0.026592939916269723\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News top comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10723"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = path + \"top_comments_news.pkl\"\n",
    "\n",
    "\n",
    "pkl_file = open(spec_path, 'rb') # from feature set.\n",
    "comments6 = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments6).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('donald', 0.7159775495529175)\n",
      "('erdogan', 0.683106005191803)\n",
      "('election', 0.6680687665939331)\n",
      "('obama', 0.6522588133811951)\n",
      "('trump', 0.6491758227348328)\n",
      "('flynn', 0.6461325287818909)\n",
      "('administration', 0.6432797312736511)\n",
      "('mueller', 0.6407721042633057)\n",
      "('candidate', 0.6254674196243286)\n",
      "('comey', 0.6114505529403687)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that you this\n",
      "\n",
      "\n",
      "Least common words:\n",
      "psu catman wham\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.2599127533802413 0.07999153167409581\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Reddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20130"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_all = comments1 + comments2 + comments3 + comments4 + comments5 + comments6\n",
    "\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(comments_all).to_string())]\n",
    "model = gensim.models.Word2Vec(sentences, iter=10, min_count=10, size=300,) # virker meget bedre med min_count = 10\n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('presi', 0.6867668628692627)\n",
      "('pres', 0.6520718336105347)\n",
      "('versa', 0.6129910945892334)\n",
      "('potus', 0.5942607522010803)\n",
      "('presidential', 0.4036596417427063)\n",
      "('trump', 0.40084153413772583)\n",
      "('appointee', 0.3959399461746216)\n",
      "('impeached', 0.39468443393707275)\n",
      "('elizondo', 0.39404264092445374)\n",
      "('erdogan', 0.39086031913757324)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "that you this\n",
      "\n",
      "\n",
      "Least common words:\n",
      "psu catman wham\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.2835504692155338 0.04269146825776404\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Aggrigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = comments_all + list(df_unlabled_big1.text) + list(df_unlabled_big2.text) + list(df_unlabled.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539464"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [prep2(sent) for sent in nltk.sent_tokenize(pd.Series(all_text).to_string())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38157"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this time we set a bit more options:\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 300    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# tweets are back from 140 character limit; not 240!\n",
    "model = gensim.models.Word2Vec(sentences, size=feature_size, window=window_context, min_count=min_word_count, sample=sample, iter=50)\n",
    "                               \n",
    "len(list(model.wv.vocab.keys())) # Men, med min_count = 10 har du kun 1650 obs... Flere tweets mere text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to president:\n",
      "\n",
      "('administration', 0.6124956607818604)\n",
      "('pres', 0.6078202128410339)\n",
      "('potus', 0.48590153455734253)\n",
      "('admin', 0.4723565876483917)\n",
      "('presi', 0.3922380805015564)\n",
      "('donald', 0.37990128993988037)\n",
      "('president-elect', 0.3515026569366455)\n",
      "('trump', 0.3480735421180725)\n",
      "('realdonaldtrump', 0.32141876220703125)\n",
      "('versa', 0.3123326301574707)\n",
      "\n",
      "\n",
      "Most common words:\n",
      "you that this\n",
      "\n",
      "\n",
      "Least common words:\n",
      "peterwsj bkesling opportunityagenda\n",
      "\n",
      "\n",
      "Sanity sim check1:\n",
      "0.0604121496896793 -0.0734339004365574\n",
      "\n",
      "\n",
      "Sanity sim check2:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "check_model(model, \"president\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('word2vec_model_back_up.pkl', 'wb')\n",
    "pickle.dump(model, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.word_vec(\"potus\").shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
